{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall detection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uclasystem/VQPy/blob/main/examples/fall_detection/demo.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this example we use models in [Human Falling Detection and Tracking](https://github.com/GajuuzZ/Human-Falling-Detect-Tracks) to track human movement and detect action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Python3.8 is recommended to avoid compatibility issues when installing YOLOX.\n",
    "\n",
    "Run the block below to install YOLOX and other dependencies, download YOLOX checkpoints, and download VQPy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install YOLOX\n",
    "!git clone https://github.com/Megvii-BaseDetection/YOLOX.git\n",
    "# download YOLOX pretrained model\n",
    "!wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_x.pth\n",
    "!cd YOLOX && pip3 install .\n",
    "# download VQPy, move vqpy/ to root directory for import\n",
    "!git clone https://github.com/uclasystem/VQPy.git\n",
    "!mv VQPy/vqpy ./\n",
    "# install VQPy's dependencies\n",
    "!pip3 install lap cython_bbox shapely"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download video from [here](https://youtu.be/ctniCxIdpTY) and place it in the same directory as this notebook.\n",
    "\n",
    "You'll also need to download pre-trained models yourself from [SPPE FastPose (AlphaPose)](https://drive.google.com/file/d/1IPfCDRwCmQDnQy94nT1V-_NVtTEi4VmU/view?usp=sharing) and [ST-GCN](https://drive.google.com/file/d/1mQQ4JHe58ylKbBqTjuKzpwN2nwKOWJ9u/view?usp=sharing) and place them in the same directory. (models from [Human Falling Detection and Tracking](https://github.com/GajuuzZ/Human-Falling-Detect-Tracks#pre-trained-models))\n",
    "\n",
    "For reference, the working directory should look like:\n",
    "\n",
    "```text\n",
    ".\n",
    "├── VQPy\n",
    "├── YOLOX\n",
    "├── fall.mp4\t# video to query on\n",
    "├── vqpy\t# make vqpy available for import\n",
    "├── yolox_x.pth\t# YOLOX model checkpoint\n",
    "├── fast_res50_256x192.pth\t# AlphaPose model checkpoint\n",
    "└── tsstg-model.pth\t#ST-GCN model checkpoint\n",
    "```\n",
    "\n",
    "Check all required files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [\"fall.mp4\", \"fast_res50_256x192.pth\", \"tsstg-model.pth\"];\n",
    "non_exist = [f for f in files if not os.path.isfile(f)];\n",
    "print(\"missing files: %s\" % non_exist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"./fall.mp4\"\t# path to video\n",
    "model_dir = \"./\"\t# path to pretrained models, for both object detector and pose detection\n",
    "save_folder = \"./vqpy_outputs\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fall detection with VQPy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define `VObj` type for person"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in people's pose, we need to create a `Person` VObj."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pose prediction\n",
    "\n",
    "Two models are used to predict the pose a person:\n",
    "\n",
    "- [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose): gets person's body keypoints from cropped image of person. Specifically, takes the frame and person's bounding box, returns a list of keypoints. Keypoints are mid-products to be used in ST-GCN.\n",
    "- [ST-GCN](https://github.com/yysijie/st-gcn): predict action from every 30 frames of keypoints. Takes list of keypoints, returns pose predicted, includes 7 actions: `\"Standing\", \"Walking\", \"Sitting\", \"Lying Down\", \"Stand up\", \"Sit down\", \"Fall Down\"`.\n",
    "\n",
    "To store the final output, person's pose, and mid-product, list of keypoints, we create two properties in `Person` VObj.\n",
    "\n",
    "Since ST-GCN requires keypoints be stored for the last 30 frames, function that computes `keypoints` needs to be decorated with `@stateful(30)`, where `30` specifies that 30 frames of values should be saved.\n",
    "\n",
    "Adding the two properties to `Person`, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import torch, os, numpy as np, sys\n",
    "sys.path.append(\"VQPy/examples/fall_detection/detect\")\n",
    "# import vqpy\n",
    "import vqpy\n",
    "# import AlphaPose and ST-GCN models\n",
    "from PoseEstimateLoader import SPPE_FastPose\n",
    "from ActionsEstLoader import TSSTG\n",
    "\n",
    "class Person(vqpy.VObjBase):\n",
    "    required_fields = ['class_id', 'tlbr']\n",
    "\n",
    "    # loading the two models for inference\n",
    "    pose_model = SPPE_FastPose('resnet50', 224, 160, device='cuda',\n",
    "            weights_file=os.path.join(model_dir, \"fast_res50_256x192.pth\")\n",
    "    )\n",
    "    action_model = TSSTG(\n",
    "        weight_file=os.path.join(model_dir, \"tsstg-model.pth\")\n",
    "    )\n",
    "\n",
    "    @vqpy.property()\n",
    "    @vqpy.stateful(30)  # require 30 frames of \n",
    "    def keypoints(self):\n",
    "        image = self._ctx.frame\n",
    "        tlbr = self.getv('tlbr')\n",
    "        # per-frame property, tlbr could be None when tracking is lost\n",
    "        # temporary work around until we have better dependency control\n",
    "        if tlbr is None:\n",
    "            return None\n",
    "        return Person.pose_model.predict(image, torch.tensor([tlbr]))\n",
    "\n",
    "    @vqpy.property()\n",
    "    def pose(self) -> str:\n",
    "        keypoints_list = []\n",
    "        # retrieve list of keypoints from the last 30 frames\n",
    "        # also need to deal with object lost during tracking\n",
    "        # return 'unknown' if not enough keypoints\n",
    "        for i in range(-self._track_length, 0):\n",
    "            keypoint = self.getv('keypoints', i)\n",
    "            if keypoint is not None:\n",
    "                keypoints_list.append(keypoint)\n",
    "            if len(keypoints_list) >= 30:\n",
    "                break\n",
    "        if len(keypoints_list) < 30:\n",
    "            return 'unknown'\n",
    "        # type conversion to adapt data to model input\n",
    "        pts = np.array(keypoints_list, dtype=np.float32)\n",
    "        out = Person.action_model.predict(pts, self._ctx.frame.shape[:2])\n",
    "        action_name = Person.action_model.class_names[out[0].argmax()]\n",
    "        return action_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Query on `Person`'s pose\n",
    "\n",
    "To filter on people that are falling down, we filter on `pose` having value `\"Fall Down\"` (7 actions should be supported: `\"Standing\", \"Walking\", \"Sitting\", \"Lying Down\", \"Stand up\", \"Sit down\", \"Fall Down\"`).\n",
    "\n",
    "`filter_cons` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cons = {\n",
    "    '__class__': lambda x: x == Person,\n",
    "    'pose': lambda x: x == \"Fall Down\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For output, we select:\n",
    "\n",
    "- tracker id, selected with `track_id`\n",
    "- bounding box, in format of coordinates of top-left and bottom-right corner, selected with `tlbr`. Need to be converted to `str` before serializing.\n",
    "\n",
    "`select_cons` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cons = {\n",
    "    'track_id': None,\n",
    "    'tlbr': lambda x: str(x)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FallDetection(vqpy.QueryBase):\n",
    "    @staticmethod\n",
    "    def setting() -> vqpy.VObjConstraint:\n",
    "        return vqpy.VObjConstraint(\n",
    "            filter_cons=filter_cons,\n",
    "            select_cons=select_cons,\n",
    "            filename='fall'\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the query\n",
    "\n",
    "With the `Person` VObj and the query defined, we can run the query, with:\n",
    "\n",
    "- `cls_name` is a tuple for mapping numerical outputs of object detector to literal detection class name\n",
    "\n",
    "\tHere we use `COCO_CLASSES` since it includes all the class names of interest in the fall detection query, i.e. `\"person\".\n",
    "\n",
    "- dictionary `cls_type` is then used to map detection class name (in str) to VObj types defined\n",
    "\n",
    "\t`{\"person\": Person}` means we wish to map COCO class `person` to VObj type `Person`\n",
    "\n",
    "- `tasks` is a list of queries to run on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqpy.launch(\n",
    "    cls_name=vqpy.COCO_CLASSES,\n",
    "    cls_type={\"person\": Person},\n",
    "    tasks=[FallDetection()],\n",
    "    video_path=video_path,\n",
    "    save_folder=save_folder,\n",
    "    detector_model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the query will be in `{save_folder}/{video_name}_{task_name}_{detector_name}.json`, output for this example should be in `./vqpy_outputs/fall_fall_yolox.json`.\n",
    "\n",
    "One entry is created for each frame that has filter condition satisfied.\n",
    "\n",
    "e.g. The entry in frame 133:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"frame_id\": 133,\n",
    "  \"data\": [\n",
    "    { \"track_id\": 188, \"tlbr\": \"[485. 270. 796. 588.]\" }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "<img src=\"./demo.assets/fall133.png\" alt=\"with coordinate marked\" style=\"zoom: 60%;\" />"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
